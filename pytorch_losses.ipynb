{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The only imports we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbpresent": {
     "id": "6a2ffea5-955a-47cf-ae1d-4ead0eb3aaa6"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a loss?\n",
    "\n",
    "Also known as a utitlity function, criterion, objective function\n",
    "\n",
    "It tell us how wrong the output of some model is compared to the ground truth (the real answers). How \"badly\" or \"good\" the algorithm is doing. They come up again and again in economics, mathemtics, optimisation, statistics, ML/DL/AI etc.\n",
    "\n",
    "e.g. image of cat goes in, model says with 80% confidence it is a cat, and 20% it is a dog. Model is X Loss wrong!. \n",
    "X turns out to depend on the specific loss function but the specific number is not important. \n",
    "0 loss occurs when the model is perfect e.g. The model says 100% cat when image *actually* is a cat\n",
    "\n",
    "We won't be talking too much or at all about specific models like Neural Networks or Random Forests here, because Loss functions usually work on outputs of models regardless of the model. just purely inputs to loss functions to get the right intuitions\n",
    "\n",
    "todo show some loss curves and graphs and landscapes pictures here\n",
    "\n",
    "todo: stress the importance of this: Oriol says: \"Architectures, Losses and inputs/outputs\". These are the three main things in Deep Learning. Messenger comment here. \n",
    "\n",
    "e.g. due to the importance of the losses, I will focus a few videos on this. Anyone who studies and watches these videos will be very fluent in these.\n",
    "\n",
    "I'm also doing this for my own understanding\n",
    "\n",
    "Every loss function therefore needs inputs. These are usually the ground truth *targets* and the model outputs in Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "45525c2b-73fa-48e0-91dd-ae8f02920769"
    }
   },
   "source": [
    "## Declare Inputs and targets\n",
    "\n",
    "Every loss function needs inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbpresent": {
     "id": "b53c17d0-67f1-4609-a3a0-fc0c40d96709"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_regression: [1. 2. 3. 4. 5.]\n",
      "input_classification: [[1. 1. 1.]\n",
      " [2. 2. 2.]\n",
      " [3. 3. 3.]]\n",
      "target_regression: [1. 2. 3. 4. 6.]\n",
      "target_classification: [1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "input_regression = torch.Tensor([1, 2, 3, 4, 5])\n",
    "target_regression = torch.Tensor([1, 2, 3, 4, 6])\n",
    "\n",
    "input_classification = torch.Tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]]).transpose(1, 0)\n",
    "target_classification = torch.LongTensor([1, 2, 3, 4, 5]) # torch.LongTensor(3).random_(5)\n",
    "\n",
    "print('input_regression:', input_regression.numpy())\n",
    "print('input_classification:', input_classification.numpy())\n",
    "print('target_regression:', target_regression.numpy())\n",
    "print('target_classification:', target_classification.numpy())\n",
    "# todo print these in numpy or clearer\n",
    "# todo show math in markdown\n",
    "# work through each methodically and clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official docs\n",
    "# size_average – By default, the losses are averaged over observations for each minibatch. \n",
    "#               However, if the field size_average is set to False, the losses are instead \n",
    "#               summed for each minibatch. Only applies when reduce is True. Default: True\n",
    "# reduce – By default, the losses are averaged over observations for each minibatch, \n",
    "#         or summed, depending on size_average. When reduce is False, \n",
    "#         returns a loss per input/target element instead \n",
    "#         and ignores size_average. Default: True\n",
    "\n",
    "\n",
    "# Two different params in common for ALL loss functions in PyTorch with defaults set to: \n",
    "# reduce=True and size_average=True\n",
    "# size_average only matters when reduce=True and means we will average, otherwise we only sum\n",
    "# reduce default is True, but if False, we will get [0. 0. 0. 0. 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1Loss AKA absolute loss AKA Laplace\n",
    "\n",
    "$$ L = \\sum_{i=0}^n \\left| y_i - h(x_i) \\right|$$\n",
    "\n",
    "$$\\frac{(1 - 1) + (2 - 2) + (3 - 3) + (4 - 4) + (6 - 5)}{5} =\\frac{1}{5} = 0.2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbpresent": {
     "id": "e262d00b-ce3e-4172-ac86-00d5a8462569"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [1. 2. 3. 4. 5.]\n",
      "Target:  [1. 2. 3. 4. 6.] \n",
      "\n",
      "L1Loss default (avg reduce) [0.2]\n",
      "L1Loss no average (sum reduce): [1.]\n",
      "L1Loss: no reduce: [0. 0. 0. 0. 1.]\n",
      "\n",
      "Show Gradients for loss\n",
      "L1Loss: [0.2]\n",
      "[ 0.2  0.2  0.2  0.2 -0.2]\n",
      "\n",
      " 0.8000\n",
      " 1.8000\n",
      " 2.8000\n",
      " 3.8000\n",
      " 5.2000\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# L1Loss AKA absolute loss\n",
    "loss_default = nn.L1Loss()\n",
    "loss_sum_reduce = nn.L1Loss(size_average=False)\n",
    "loss_no_reduce = nn.L1Loss(reduce=False, size_average=True) # Doesn't average by the number of elements\n",
    "\n",
    "inp = Variable(input_regression, requires_grad=True) # todo requires necessary?\n",
    "target = Variable(target_regression)\n",
    "print('Input: ', inp.data.numpy())\n",
    "print('Target: ', target.data.numpy(), '\\n')\n",
    "print('L1Loss default (avg reduce) {}'.format(loss_default(inp, target).data.numpy())) # example above\n",
    "print('L1Loss no average (sum reduce): {}'.format(loss_sum(inp, target).data.numpy())) # numerator in fraction\n",
    "print('L1Loss: no reduce: {}'.format(loss_no_reduce(inp, target).data.numpy())) # loss for each example\n",
    "\n",
    "# Show gradients. todo explain and show output gradients. \n",
    "output = loss_default(inp, target)\n",
    "output.backward()\n",
    "print('\\nShow Gradients for loss')\n",
    "print('L1Loss: {}'.format(output.data.numpy()))\n",
    "print(inp.grad.data.numpy())\n",
    "\n",
    "print(inp.data - inp.grad.data) # todo why are they all 0.2 when they were correct?\n",
    "\n",
    "# We want the user to get a real intuition for how the final layer is wrong.\n",
    "# Todo quantile regression loss and Squared loss (without importance weight aware updates)\n",
    "# ridge and lasso regression. mention regularisation\n",
    "# todo add more english from official docstrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE Loss AKA Euclidean Distance AKA AKA\n",
    "\n",
    "$$ L = \\sum_0^n (y_i - h(x_i))^2 $$\n",
    "\n",
    "$$\\frac{(1 - 1)^2 + (2 - 2)^2 + (3 - 3)^2 + (4 - 4)^2 + (6 - 5)^2}{5} =\\frac{1}{5} = 0.2 $$\n",
    "\n",
    "L2 Loss is top of fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbpresent": {
     "id": "2fcdd82e-9bde-493a-afda-7ba504b542a6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss: [0.2]\n",
      "\n",
      " 1.0000\n",
      " 2.0000\n",
      " 3.0000\n",
      " 4.0000\n",
      " 5.4000\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MSE Loss AKA AKA AKA\n",
    "loss = nn.MSELoss() # default paramaters (reduce and size_average) are the same as for L1Loss and the others\n",
    "inp = Variable(input_regression, requires_grad=True)\n",
    "target = Variable(target_regression)\n",
    "output = loss(inp, target)\n",
    "output.backward()\n",
    "print('MSELoss: {}'.format(output.data.numpy()))\n",
    "\n",
    "print(inp.data - inp.grad.data)\n",
    "\n",
    "# todo add pros and cons for L1 vs L2 Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmoothL1Loss AKA Huber loss # Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise.\n",
    "loss = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification losses\n",
    "\n",
    "## CrossEntropyLoss\n",
    "\n",
    "https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/\n",
    "A Short Introduction to Entropy, Cross-Entropy and KL-Divergence  \n",
    "https://www.youtube.com/watch?v=ErfnhcEV1O8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbpresent": {
     "id": "99a29203-b507-40ef-9989-d590302bde85"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.9194e-01  1.0000e-04  1.0000e-04  7.7704e-01  1.2983e+00\n",
      " 1.0940e+00  9.3695e-02  1.4648e+00  1.0000e-04  1.0000e-04\n",
      "[torch.FloatTensor of size 2x5]\n",
      " Variable containing:\n",
      " 1\n",
      " 2\n",
      "[torch.LongTensor of size 2]\n",
      "\n",
      "CrossEntropyLoss: Variable containing:\n",
      " 1.5474\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CrossEntropyLoss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "#loss = nn.BCELoss()\n",
    "#inp = Variable(input_classification, requires_grad=True)\n",
    "#target = Variable(target_classification)\n",
    "\n",
    "num_rows = 2\n",
    "num_classes = 5\n",
    "inp = Variable(torch.randn(num_rows, num_classes).clamp(0.0001, 100), requires_grad=True)\n",
    "target = Variable(torch.LongTensor(num_rows).random_(num_classes))\n",
    "print(inp, target)\n",
    "output = loss(inp, target)\n",
    "output.backward()\n",
    "#print(target.data.numpy().flatten()[0])\n",
    "#print(-np.log(inp[target.data.numpy().flatten()[0]]))\n",
    "print('CrossEntropyLoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "nbpresent": {
     "id": "b3966226-29fc-461d-9ecb-81c4c365205f"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLLLoss: Variable containing:\n",
      " 2.1415\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beduffy/anaconda/envs/rl-env/lib/python3.6/site-packages/ipykernel/__main__.py:8: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# NLLLoss\n",
    "m = nn.LogSoftmax()\n",
    "loss = nn.NLLLoss()\n",
    "# inp is of size N x C = 3 x 5\n",
    "inp = Variable(torch.randn(3, 5), requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = Variable(torch.LongTensor([1, 0, 4]))\n",
    "output = loss(m(inp), target)\n",
    "output.backward()\n",
    "print('NLLLoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nbpresent": {
     "id": "335f3f51-af89-41b4-b642-49aee8c45904"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoissonNLLLoss: Variable containing:\n",
      " 1.3639\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PoissonNLLLoss # Negative log likelihood loss with Poisson distribution of target.\n",
    "loss = nn.PoissonNLLLoss()\n",
    "log_inp = Variable(torch.randn(5, 2), requires_grad=True)\n",
    "target = Variable(torch.randn(5, 2))\n",
    "output = loss(log_inp, target)\n",
    "output.backward()\n",
    "print('PoissonNLLLoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "nbpresent": {
     "id": "8821a253-42bc-4468-87ee-ac0400fcb5a0"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLLLoss2d: Variable containing:\n",
      "1.00000e-02 *\n",
      " -2.9927\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLLLoss2d # negative log likehood loss, but for image inputs. It computes NLL loss per-pixel.\n",
    "m = nn.Conv2d(16, 32, (3, 3)).float()\n",
    "loss = nn.NLLLoss2d()\n",
    "# input is of size N x C x height x width\n",
    "inp = Variable(torch.randn(3, 16, 10, 10))\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = Variable(torch.LongTensor(3, 8, 8).random_(0, 4))\n",
    "output = loss(m(inp), target)\n",
    "output.backward()\n",
    "print('NLLLoss2d: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "nbpresent": {
     "id": "6d4cddf1-95b2-409b-a3ac-35c1bfc22c9d"
    }
   },
   "outputs": [],
   "source": [
    "# KLDivLoss # The Kullback-Leibler divergence Loss\n",
    "loss = nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "nbpresent": {
     "id": "d4b952dd-aca1-4fda-8a04-a620431689c2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "m(inp (output)): Variable containing:\n",
      " 0.6500\n",
      " 0.3623\n",
      " 0.7615\n",
      "[torch.FloatTensor of size 3]\n",
      " BCELoss: Variable containing:\n",
      " 1.7185\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BCELoss # Binary Cross Entropy\n",
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss(size_average=False) # default is True\n",
    "inp = Variable(torch.randn(3), requires_grad=True)\n",
    "target = Variable(torch.FloatTensor(3).random_(2))\n",
    "output = loss(m(inp), target)\n",
    "output.backward()\n",
    "print('target:', target)\n",
    "print('m(inp (output)): {} BCELoss: {}'.format(m(inp), output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "nbpresent": {
     "id": "facd0090-1e8f-425b-bf28-9af53ce8ddd8"
    }
   },
   "outputs": [],
   "source": [
    "# BCEWithLogitsLoss # This loss combines a Sigmoid layer and the BCELoss in one single class\n",
    "loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "nbpresent": {
     "id": "f1854bed-5655-4b60-8f56-92540b6ef31a"
    }
   },
   "outputs": [],
   "source": [
    "# MarginRankingLoss # Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch Tensor`s, and a label 1D mini-batch tensor `y with values (1 or -1).\n",
    "loss = nn.MarginRankingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "nbpresent": {
     "id": "a8afbb59-5cc9-4818-b7f8-4f0206640940"
    }
   },
   "outputs": [],
   "source": [
    "# HingeEmbeddingLoss\n",
    "'''                 { x_i,                  if y_i ==  1\n",
    "loss(x, y) = 1/n {\n",
    "                    { max(0, margin - x_i), if y_i == -1'''\n",
    "\n",
    "loss = nn.HingeEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "nbpresent": {
     "id": "223163e4-7af4-455e-bdc0-62d63d4f89ae"
    }
   },
   "outputs": [],
   "source": [
    "#MultiLabelMarginLoss # multi-class multi-classification hinge loss (margin-based loss) \n",
    "# loss(x, y) = sum_ij(max(0, 1 - (x[y[j]] - x[i]))) / x.size(0)\n",
    "\n",
    "loss = nn.MultiLabelMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "nbpresent": {
     "id": "8362555b-d083-4f19-814c-51bb54fa0816"
    }
   },
   "outputs": [],
   "source": [
    "# SoftMarginLoss # two-class classification logistic loss\n",
    "loss = nn.SoftMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbpresent": {
     "id": "98caf50e-39cc-4be6-8cc8-222ec020ad6d"
    }
   },
   "outputs": [],
   "source": [
    "# MultiLabelSoftMarginLoss # multi-label one-versus-all loss based on max-entropy\n",
    "loss = nn.MultiLabelSoftMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "nbpresent": {
     "id": "e609faf4-a640-4d34-82dd-4eb8c9226a20"
    }
   },
   "outputs": [],
   "source": [
    "# CosineEmbeddingLoss\n",
    "loss = nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "nbpresent": {
     "id": "ef4d1ca7-fe42-44bb-975f-f9063feb5c41"
    }
   },
   "outputs": [],
   "source": [
    "# MultiMarginLoss\n",
    "loss = nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "nbpresent": {
     "id": "7b5645fa-d7fa-44e2-a730-234ffde9d0ba"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Variable containing:\n",
      " 1.2470\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TripletMarginLoss\n",
    "\n",
    "\"\"\"\n",
    "Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3 and a margin with a value greater than 0. This is used for measuring a relative similarity between samples. A triplet is composed by a, p and n: anchor, positive examples and negative example respectively. The shapes of all input tensors should be (N,D).\n",
    "\"\"\"\n",
    "# defaults: margin=1.0, p=2\n",
    "triplet_loss = nn.TripletMarginLoss()\n",
    "input1 = Variable(torch.randn(100, 128), requires_grad=True)\n",
    "input2 = Variable(torch.randn(100, 128), requires_grad=True)\n",
    "input3 = Variable(torch.randn(100, 128), requires_grad=True)\n",
    "output = triplet_loss(input1, input2, input3)\n",
    "print(output.backward())\n",
    "print(output)\n",
    "# todo push to pytorch official docs that the example they have is wrong. They don't wrap the tensors in variables. But that is solved in 0.4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "15052208-e02e-4256-83a7-640de87e1864"
    }
   },
   "source": [
    "# Loss functions in every single relevant framework\n",
    "\n",
    "You should now understand 90% of the loss functions in the below frameworks. \n",
    "\n",
    "**PyTorch Losses**: http://pytorch.org/docs/master/nn.html#loss-functions\n",
    "\n",
    "**Torch Losses**: https://github.com/torch/nn/blob/master/doc/criterion.md\n",
    "\n",
    "**Keras Losses**: https://keras.io/losses/\n",
    "\n",
    "**TensorFlow Losses**: https://www.tensorflow.org/api_docs/python/tf/losses\n",
    "\n",
    "**Gluon/MXNet Losses**: https://mxnet.incubator.apache.org/api/python/gluon/loss.html\n",
    "\n",
    "**Chainer Losses**: http://docs.chainer.org/en/stable/reference/functions.html#loss-functions\n",
    "\n",
    "**CNTK Losses**: https://docs.microsoft.com/en-us/cognitive-toolkit/Loss-Functions-and-Metrics\n",
    "\n",
    "**DeepLearning4j Losses**: https://deeplearning4j.org/features#lossobjective-functions\n",
    "\n",
    "**Lasagne Losses**: http://lasagne.readthedocs.io/en/latest/modules/objectives.html\n",
    "\n",
    "**PaddlePaddle Losses**: http://paddlepaddle.org/docs/develop/api/en/v2/config/layer.html?highlight=loss#cost-layers\n",
    "\n",
    "**Caffe2 Losses**: Couldn't find a good and simple list for Caffe or Caffe2\n",
    "\n",
    "### Other Resources:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Loss_function  \n",
    "https://en.wikipedia.org/wiki/Loss_functions_for_classification  \n",
    "http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html  \n",
    "https://davidrosenberg.github.io/ml2015/docs/3a.loss-functions.pdf  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-env]",
   "language": "python",
   "name": "conda-env-rl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
