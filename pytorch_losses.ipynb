{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The only imports we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "6a2ffea5-955a-47cf-ae1d-4ead0eb3aaa6"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "45525c2b-73fa-48e0-91dd-ae8f02920769"
    }
   },
   "source": [
    "## Declare Inputs and targets\n",
    "\n",
    "Every loss function needs inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "b53c17d0-67f1-4609-a3a0-fc0c40d96709"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_regression: \n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "input_classification: \n",
      " 1  1  1\n",
      " 2  2  2\n",
      " 3  3  3\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "target_regression: \n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 6\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "target_classification: \n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      "[torch.LongTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_regression = torch.Tensor([1, 2, 3, 4, 5])\n",
    "target_regression = torch.Tensor([1, 2, 3, 4, 6])\n",
    "\n",
    "input_classification = torch.Tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]]).transpose(1, 0)\n",
    "target_classification = torch.LongTensor([1, 2, 3, 4, 5]) # torch.LongTensor(3).random_(5)\n",
    "\n",
    "print('input_regression:', input_regression)\n",
    "print('input_classification:', input_classification)\n",
    "print('target_regression:', target_regression)\n",
    "print('target_classification:', target_classification)\n",
    "# todo print these in numpy or clearer\n",
    "# todo show math in markdown\n",
    "# work through each methodically and clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1Loss AKA absolute loss\n",
    "\n",
    "\\begin{align}\n",
    "\\sum \\left|y_i - h(x_i)\\right|\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbpresent": {
     "id": "e262d00b-ce3e-4172-ac86-00d5a8462569"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      "[torch.FloatTensor of size 5]\n",
      " Variable containing:\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 6\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'size_average'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-067e3ac1ac89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_regression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#output.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L1Loss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/rl-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'size_average'"
     ]
    }
   ],
   "source": [
    "# L1Loss AKA absolute loss\n",
    "#loss = nn.L1Loss(reduce=False) # Shows loss for each single value\n",
    "#loss = nn.L1Loss()\n",
    "loss = nn.L1Loss(size_average=False) # Doesn't average by the number of elements\n",
    "input = autograd.Variable(input_regression, requires_grad=True) # todo requires necessary?\n",
    "target = autograd.Variable(target_regression)\n",
    "print(input, target)\n",
    "output = loss(input, target)\n",
    "#output.backward()\n",
    "print('L1Loss: {}'.format(output))\n",
    "\n",
    "# todo explain and show output gradients. \n",
    "# We want the user to get a real intuition for how the final layer is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "2fcdd82e-9bde-493a-afda-7ba504b542a6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss: Variable containing:\n",
      " 0.2000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MSE Loss AKA AKA AKA\n",
    "loss = nn.MSELoss()\n",
    "input = autograd.Variable(input_regression, requires_grad=True)\n",
    "target = autograd.Variable(target_regression)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('MSELoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "99a29203-b507-40ef-9989-d590302bde85"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss: Variable containing:\n",
      " 1.9333\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CrossEntropyLoss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "#input = autograd.Variable(input_classification, requires_grad=True)\n",
    "#target = autograd.Variable(target_classification)\n",
    "input = autograd.Variable(torch.randn(3, 5), requires_grad=True)\n",
    "target = autograd.Variable(torch.LongTensor(3).random_(5))\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('CrossEntropyLoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbpresent": {
     "id": "b3966226-29fc-461d-9ecb-81c4c365205f"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLLLoss: Variable containing:\n",
      " 2.7543\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beduffy/anaconda/envs/rl-env/lib/python3.6/site-packages/ipykernel/__main__.py:8: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# NLLLoss\n",
    "m = nn.LogSoftmax()\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = autograd.Variable(torch.randn(3, 5), requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = autograd.Variable(torch.LongTensor([1, 0, 4]))\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "print('NLLLoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbpresent": {
     "id": "335f3f51-af89-41b4-b642-49aee8c45904"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoissonNLLLoss: Variable containing:\n",
      " 0.7165\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PoissonNLLLoss # Negative log likelihood loss with Poisson distribution of target.\n",
    "loss = nn.PoissonNLLLoss()\n",
    "log_input = autograd.Variable(torch.randn(5, 2), requires_grad=True)\n",
    "target = autograd.Variable(torch.randn(5, 2))\n",
    "output = loss(log_input, target)\n",
    "output.backward()\n",
    "print('PoissonNLLLoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbpresent": {
     "id": "8821a253-42bc-4468-87ee-ac0400fcb5a0"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLLLoss2d: Variable containing:\n",
      "1.00000e-02 *\n",
      " -1.0574\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLLLoss2d # negative log likehood loss, but for image inputs. It computes NLL loss per-pixel.\n",
    "m = nn.Conv2d(16, 32, (3, 3)).float()\n",
    "loss = nn.NLLLoss2d()\n",
    "# input is of size N x C x height x width\n",
    "input = autograd.Variable(torch.randn(3, 16, 10, 10))\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = autograd.Variable(torch.LongTensor(3, 8, 8).random_(0, 4))\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "print('NLLLoss2d: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbpresent": {
     "id": "6d4cddf1-95b2-409b-a3ac-35c1bfc22c9d"
    }
   },
   "outputs": [],
   "source": [
    "# KLDivLoss # The Kullback-Leibler divergence Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbpresent": {
     "id": "d4b952dd-aca1-4fda-8a04-a620431689c2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCELoss: Variable containing:\n",
      " 0.6185\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BCELoss # Binary Cross Entropy\n",
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = autograd.Variable(torch.randn(3), requires_grad=True)\n",
    "target = autograd.Variable(torch.FloatTensor(3).random_(2))\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "print('BCELoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbpresent": {
     "id": "facd0090-1e8f-425b-bf28-9af53ce8ddd8"
    }
   },
   "outputs": [],
   "source": [
    "# BCEWithLogitsLoss # This loss combines a Sigmoid layer and the BCELoss in one single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbpresent": {
     "id": "f1854bed-5655-4b60-8f56-92540b6ef31a"
    }
   },
   "outputs": [],
   "source": [
    "# MarginRankingLoss # Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch Tensor`s, and a label 1D mini-batch tensor `y with values (1 or -1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "a8afbb59-5cc9-4818-b7f8-4f0206640940"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                 { x_i,                  if y_i ==  1\\nloss(x, y) = 1/n {\\n                    { max(0, margin - x_i), if y_i == -1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HingeEmbeddingLoss\n",
    "'''                 { x_i,                  if y_i ==  1\n",
    "loss(x, y) = 1/n {\n",
    "                    { max(0, margin - x_i), if y_i == -1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbpresent": {
     "id": "223163e4-7af4-455e-bdc0-62d63d4f89ae"
    }
   },
   "outputs": [],
   "source": [
    "#MultiLabelMarginLoss # multi-class multi-classification hinge loss (margin-based loss) \n",
    "# loss(x, y) = sum_ij(max(0, 1 - (x[y[j]] - x[i]))) / x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbpresent": {
     "id": "381d5705-c97d-416d-9873-452bc3e0ea52"
    }
   },
   "outputs": [],
   "source": [
    "# SmoothL1Loss AKA Huber loss # Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbpresent": {
     "id": "8362555b-d083-4f19-814c-51bb54fa0816"
    }
   },
   "outputs": [],
   "source": [
    "# SoftMarginLoss # two-class classification logistic loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbpresent": {
     "id": "98caf50e-39cc-4be6-8cc8-222ec020ad6d"
    }
   },
   "outputs": [],
   "source": [
    "# MultiLabelSoftMarginLoss # multi-label one-versus-all loss based on max-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbpresent": {
     "id": "e609faf4-a640-4d34-82dd-4eb8c9226a20"
    }
   },
   "outputs": [],
   "source": [
    "# CosineEmbeddingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbpresent": {
     "id": "ef4d1ca7-fe42-44bb-975f-f9063feb5c41"
    }
   },
   "outputs": [],
   "source": [
    "# MultiMarginLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbpresent": {
     "id": "7b5645fa-d7fa-44e2-a730-234ffde9d0ba"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\\ninput1 = autograd.Variable(torch.randn(100, 128))\\ninput2 = autograd.Variable(torch.randn(100, 128))\\ninput3 = autograd.Variable(torch.randn(100, 128))\\noutput = triplet_loss(input1, input2, input3)\\noutput.backward()'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TripletMarginLoss\n",
    "'''triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "input1 = autograd.Variable(torch.randn(100, 128))\n",
    "input2 = autograd.Variable(torch.randn(100, 128))\n",
    "input3 = autograd.Variable(torch.randn(100, 128))\n",
    "output = triplet_loss(input1, input2, input3)\n",
    "output.backward()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "15052208-e02e-4256-83a7-640de87e1864"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-env]",
   "language": "python",
   "name": "conda-env-rl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
