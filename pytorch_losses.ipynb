{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The only imports we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "6a2ffea5-955a-47cf-ae1d-4ead0eb3aaa6"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "45525c2b-73fa-48e0-91dd-ae8f02920769"
    }
   },
   "source": [
    "## Declare Inputs and targets\n",
    "\n",
    "Every loss function needs inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbpresent": {
     "id": "b53c17d0-67f1-4609-a3a0-fc0c40d96709"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_regression: \n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "input_classification: \n",
      " 1  1  1\n",
      " 2  2  2\n",
      " 3  3  3\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "target_regression: \n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 6\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "target_classification: \n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      "[torch.LongTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_regression = torch.Tensor([1, 2, 3, 4, 5])\n",
    "target_regression = torch.Tensor([1, 2, 3, 4, 6])\n",
    "\n",
    "input_classification = torch.Tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]]).transpose(1, 0)\n",
    "target_classification = torch.LongTensor([1, 2, 3, 4, 5]) # torch.LongTensor(3).random_(5)\n",
    "\n",
    "print('input_regression:', input_regression)\n",
    "print('input_classification:', input_classification)\n",
    "print('target_regression:', target_regression)\n",
    "print('target_classification:', target_classification)\n",
    "# todo print these in numpy or clearer\n",
    "# todo show math in markdown\n",
    "# work through each methodically and clearly\n",
    "\n",
    "# todo: mention \"We won't be talking about models here\", just purely inputs to loss functions to get the right intuitions\n",
    "# todo show some loss curves and graphs and landscapes\n",
    "# todo: stress the importance of this: Oriol says: \"Architectures, Losses and inputs/outputs\"\n",
    "# e.g. due to the importance of the losses, I will focus a few videos on this. Anyone who studies and watches these will be very fluent in these.\n",
    "# I'm also doing this for my own understanding\n",
    "# todo add more english from official docstrings\n",
    "# AKA utitlity function, criterion, objective, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1Loss AKA absolute loss AKA Laplace\n",
    "\n",
    "$$ L = \\sum_{i=0}^n \\left| y_i - h(x_i) \\right|$$\n",
    "\n",
    "$$\\frac{(1 - 1) + (2 - 2) + (3 - 3) + (4 - 4) + (6 - 5)}{5} =\\frac{1}{5} = 0.2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbpresent": {
     "id": "e262d00b-ce3e-4172-ac86-00d5a8462569"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      "[torch.FloatTensor of size 5]\n",
      " Variable containing:\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 6\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "L1Loss: Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# L1Loss AKA absolute loss\n",
    "#loss = nn.L1Loss(reduce=False) # Shows loss for each single value\n",
    "#loss = nn.L1Loss()\n",
    "loss = nn.L1Loss(size_average=False) # Doesn't average by the number of elements\n",
    "input = autograd.Variable(input_regression, requires_grad=True) # todo requires necessary?\n",
    "target = autograd.Variable(target_regression) # todo remove \"autograd.\"\n",
    "print(input, target)\n",
    "output = loss(input, target)\n",
    "#output.backward()\n",
    "print('L1Loss: {}'.format(output))\n",
    "\n",
    "# todo explain and show output gradients. \n",
    "# We want the user to get a real intuition for how the final layer is wrong.\n",
    "# todo for first loss (this one) show the difference between reduce and size_average and compare with numerator of fraction\n",
    "# todo add pros and cons for L1 vs L2 Losses\n",
    "# Todo quantile regression loss and Squared loss (without importance weight aware updates)\n",
    "# ridge and lasso regression. mention regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE Loss AKA Euclidean Distance AKA AKA\n",
    "\n",
    "$$ L = \\sum_0^n (y_i - h(x_i))^2 $$\n",
    "\n",
    "$$\\frac{(1 - 1)^2 + (2 - 2)^2 + (3 - 3)^2 + (4 - 4)^2 + (6 - 5)^2}{5} =\\frac{1}{5} = 0.2 $$\n",
    "\n",
    "L2 Loss is top of fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbpresent": {
     "id": "2fcdd82e-9bde-493a-afda-7ba504b542a6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss: Variable containing:\n",
      " 0.2000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MSE Loss AKA AKA AKA\n",
    "loss = nn.MSELoss()\n",
    "input = autograd.Variable(input_regression, requires_grad=True)\n",
    "target = autograd.Variable(target_regression)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('MSELoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SmoothL1Loss AKA Huber loss # Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification losses\n",
    "\n",
    "## CrossEntropyLoss\n",
    "\n",
    "https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "nbpresent": {
     "id": "99a29203-b507-40ef-9989-d590302bde85"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.4432e-01  1.8557e+00  5.6689e-01  1.9026e-01  1.0000e-04\n",
      " 1.0000e-04  2.1835e-01  1.0000e-04  1.0000e-04  7.3342e-02\n",
      "[torch.FloatTensor of size 2x5]\n",
      " Variable containing:\n",
      " 1\n",
      " 2\n",
      "[torch.LongTensor of size 2]\n",
      "\n",
      "CrossEntropyLoss: Variable containing:\n",
      " 1.1301\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CrossEntropyLoss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "#loss = nn.BCELoss()\n",
    "#input = autograd.Variable(input_classification, requires_grad=True)\n",
    "#target = autograd.Variable(target_classification)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_rows = 2\n",
    "num_classes = 5\n",
    "input = autograd.Variable(torch.randn(num_rows, num_classes).clamp(0.0001, 100), requires_grad=True)\n",
    "target = autograd.Variable(torch.LongTensor(num_rows).random_(num_classes))\n",
    "print(input, target)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "#print(target.data.numpy().flatten()[0])\n",
    "#print(-np.log(input[target.data.numpy().flatten()[0]]))\n",
    "print('CrossEntropyLoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbpresent": {
     "id": "b3966226-29fc-461d-9ecb-81c4c365205f"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLLLoss: Variable containing:\n",
      " 1.7336\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjamin.duffy\\AppData\\Local\\Continuum\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# NLLLoss\n",
    "m = nn.LogSoftmax()\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = autograd.Variable(torch.randn(3, 5), requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = autograd.Variable(torch.LongTensor([1, 0, 4]))\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "print('NLLLoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nbpresent": {
     "id": "335f3f51-af89-41b4-b642-49aee8c45904"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoissonNLLLoss: Variable containing:\n",
      " 2.4805\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PoissonNLLLoss # Negative log likelihood loss with Poisson distribution of target.\n",
    "loss = nn.PoissonNLLLoss()\n",
    "log_input = autograd.Variable(torch.randn(5, 2), requires_grad=True)\n",
    "target = autograd.Variable(torch.randn(5, 2))\n",
    "output = loss(log_input, target)\n",
    "output.backward()\n",
    "print('PoissonNLLLoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbpresent": {
     "id": "8821a253-42bc-4468-87ee-ac0400fcb5a0"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLLLoss2d: Variable containing:\n",
      "1.00000e-03 *\n",
      " -8.1760\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLLLoss2d # negative log likehood loss, but for image inputs. It computes NLL loss per-pixel.\n",
    "m = nn.Conv2d(16, 32, (3, 3)).float()\n",
    "loss = nn.NLLLoss2d()\n",
    "# input is of size N x C x height x width\n",
    "input = autograd.Variable(torch.randn(3, 16, 10, 10))\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = autograd.Variable(torch.LongTensor(3, 8, 8).random_(0, 4))\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "print('NLLLoss2d: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "6d4cddf1-95b2-409b-a3ac-35c1bfc22c9d"
    }
   },
   "outputs": [],
   "source": [
    "# KLDivLoss # The Kullback-Leibler divergence Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbpresent": {
     "id": "d4b952dd-aca1-4fda-8a04-a620431689c2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCELoss: Variable containing:\n",
      " 0.5804\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BCELoss # Binary Cross Entropy\n",
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = autograd.Variable(torch.randn(3), requires_grad=True)\n",
    "target = autograd.Variable(torch.FloatTensor(3).random_(2))\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "print('BCELoss: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "facd0090-1e8f-425b-bf28-9af53ce8ddd8"
    }
   },
   "outputs": [],
   "source": [
    "# BCEWithLogitsLoss # This loss combines a Sigmoid layer and the BCELoss in one single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f1854bed-5655-4b60-8f56-92540b6ef31a"
    }
   },
   "outputs": [],
   "source": [
    "# MarginRankingLoss # Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch Tensor`s, and a label 1D mini-batch tensor `y with values (1 or -1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbpresent": {
     "id": "a8afbb59-5cc9-4818-b7f8-4f0206640940"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                 { x_i,                  if y_i ==  1\\nloss(x, y) = 1/n {\\n                    { max(0, margin - x_i), if y_i == -1'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HingeEmbeddingLoss\n",
    "'''                 { x_i,                  if y_i ==  1\n",
    "loss(x, y) = 1/n {\n",
    "                    { max(0, margin - x_i), if y_i == -1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "223163e4-7af4-455e-bdc0-62d63d4f89ae"
    }
   },
   "outputs": [],
   "source": [
    "#MultiLabelMarginLoss # multi-class multi-classification hinge loss (margin-based loss) \n",
    "# loss(x, y) = sum_ij(max(0, 1 - (x[y[j]] - x[i]))) / x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "8362555b-d083-4f19-814c-51bb54fa0816"
    }
   },
   "outputs": [],
   "source": [
    "# SoftMarginLoss # two-class classification logistic loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "98caf50e-39cc-4be6-8cc8-222ec020ad6d"
    }
   },
   "outputs": [],
   "source": [
    "# MultiLabelSoftMarginLoss # multi-label one-versus-all loss based on max-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "e609faf4-a640-4d34-82dd-4eb8c9226a20"
    }
   },
   "outputs": [],
   "source": [
    "# CosineEmbeddingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "ef4d1ca7-fe42-44bb-975f-f9063feb5c41"
    }
   },
   "outputs": [],
   "source": [
    "# MultiMarginLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "nbpresent": {
     "id": "7b5645fa-d7fa-44e2-a730-234ffde9d0ba"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\\ninput1 = autograd.Variable(torch.randn(100, 128))\\ninput2 = autograd.Variable(torch.randn(100, 128))\\ninput3 = autograd.Variable(torch.randn(100, 128))\\noutput = triplet_loss(input1, input2, input3)\\noutput.backward()'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TripletMarginLoss\n",
    "'''triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "input1 = autograd.Variable(torch.randn(100, 128))\n",
    "input2 = autograd.Variable(torch.randn(100, 128))\n",
    "input3 = autograd.Variable(torch.randn(100, 128))\n",
    "output = triplet_loss(input1, input2, input3)\n",
    "output.backward()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "15052208-e02e-4256-83a7-640de87e1864"
    }
   },
   "source": [
    "# Loss functions in every single relevant framework\n",
    "\n",
    "You should now understand 90% of the loss functions in the below frameworks. \n",
    "\n",
    "#### PyTorch Losses\n",
    "http://pytorch.org/docs/master/nn.html#loss-functions\n",
    "\n",
    "#### Torch Losses\n",
    "https://github.com/torch/nn/blob/master/doc/criterion.md\n",
    "\n",
    "#### Keras Losses\n",
    "https://keras.io/losses/\n",
    "\n",
    "#### TensorFlow Losses\n",
    "https://www.tensorflow.org/api_docs/python/tf/losses\n",
    "\n",
    "#### Gluon/MXNet Losses\n",
    "https://mxnet.incubator.apache.org/api/python/gluon/loss.html\n",
    "\n",
    "#### Chainer Losses\n",
    "http://docs.chainer.org/en/stable/reference/functions.html#loss-functions\n",
    "\n",
    "#### Caffe2 Losses\n",
    "Couldn't find a good and simple list for Caffe or Caffe2\n",
    "\n",
    "#### CNTK Losses\n",
    "https://docs.microsoft.com/en-us/cognitive-toolkit/Loss-Functions-and-Metrics\n",
    "\n",
    "#### DeepLearning4j Losses\n",
    "https://deeplearning4j.org/features#lossobjective-functions\n",
    "\n",
    "#### Lasagne Losses\n",
    "http://lasagne.readthedocs.io/en/latest/modules/objectives.html\n",
    "\n",
    "#### PaddlePaddle Losses\n",
    "http://paddlepaddle.org/docs/develop/api/en/v2/config/layer.html?highlight=loss#cost-layers\n",
    "\n",
    "### Other Resources:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Loss_function  \n",
    "https://en.wikipedia.org/wiki/Loss_functions_for_classification  \n",
    "http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html  \n",
    "https://davidrosenberg.github.io/ml2015/docs/3a.loss-functions.pdf  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
